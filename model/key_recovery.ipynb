{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7a13eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import utils.dataset_loader as dataset_loader\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fab362a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRESENT_SBOX = np.array([\n",
    "    12,5,6,11,9,0,10,13,3,14,15,8,4,7,1,2\n",
    "], dtype=np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "907c56dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_hex_string_array(array):\n",
    "    result = []\n",
    "    for item in array:\n",
    "        hex_str = str(item[0])\n",
    "        nibbles = [int(c, 16) for c in hex_str]\n",
    "        result.append(nibbles)\n",
    "    return np.array(result, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fe6b4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths \n",
    "DATASETS_PATH = '../datasets'\n",
    "PREPARED_DATASETS_PATH = '../dataset/present'\n",
    "MODELS_PATH = 'trained/PRESENT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fb9c86ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset paths\n",
    "evaluation_datasets = {\n",
    "    \"random_pt_dataset\": \"random_pt_dataset\",\n",
    "    \"fixed_dataset_1\": \"fixed_dataset_1\",\n",
    "    \"fixed_dataset_2\": \"fixed_dataset_2\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc216228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for cache at: ../datasets/cache/random_dataset_cache.pkl\n",
      "Loading datasets from cache: ../datasets/cache/random_dataset_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "random_dataset = dataset_loader.load_dataset_files_with_cache(DATASETS_PATH+'/random_dataset', cache_path=f\"{DATASETS_PATH}/cache/random_dataset_cache.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c14c408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the trace matrix\n",
    "traces = dataset_loader.get_trace_matrix(random_dataset)  # shape (n_traces, n_samples)\n",
    "\n",
    "# Get plaintexts and keys\n",
    "plaintexts_values = random_dataset['plaintexts'].values\n",
    "keys_values = random_dataset['keys'].values\n",
    "\n",
    "# Convert hex strings to nibble arrays\n",
    "plaintexts = convert_to_hex_string_array(plaintexts_values)\n",
    "keys = convert_to_hex_string_array(keys_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4e60213",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_ranges_path = os.path.join(PREPARED_DATASETS_PATH, \"ranges.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6befbd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KNOWN_KEY_HEX = \"FEDCBA0123456789\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e8bf024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_nibbles(hex_str):\n",
    "    return [int(c, 16) for c in hex_str]\n",
    "\n",
    "true_key = hex_to_nibbles(KNOWN_KEY_HEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fda49d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, 14, 13, 12, 11, 10, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40864d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_guess(idx, probs, plaintexts, traces):\n",
    "    \"\"\"\n",
    "    Performs key recovery for a single nibble using a trained model's softmax predictions.\n",
    "\n",
    "    For each possible nibble key guess (0–15), the function simulates the expected SBox\n",
    "    output using plaintexts and accumulates the model’s predicted probability (log-likelihood)\n",
    "    for each guessed output across all traces. The key guess with the highest cumulative\n",
    "    log-likelihood is selected as the most likely key.\n",
    "\n",
    "    Parameters:\n",
    "        idx (int): Target nibble index (0–15)\n",
    "        probs (np.ndarray): Softmax output from the model (n_traces, 16)\n",
    "        plaintexts (np.ndarray): Plaintext bytes or nibbles used per trace\n",
    "        traces (np.ndarray): Trace input (not directly used, for compatibility/logging)\n",
    "\n",
    "    Returns:\n",
    "        best_guess (int): The most likely key nibble\n",
    "        scores (np.ndarray): Log-likelihood scores for each key guess\n",
    "    \"\"\"\n",
    "\n",
    "    n_traces = traces.shape[0]\n",
    "    scores = np.zeros(16)\n",
    "\n",
    "    # Precompute all 16 candidate SBox outputs for each trace\n",
    "    for key_guess in range(16):\n",
    "        sbox_out = np.array([PRESENT_SBOX[pt[idx] ^ key_guess] for pt in plaintexts], dtype=np.uint8)\n",
    "        # Use advanced indexing to gather the predicted probs\n",
    "        pred_probs = probs[np.arange(n_traces), sbox_out]\n",
    "        pred_probs = np.clip(pred_probs, 1e-10, 1.0)\n",
    "        scores[key_guess] = np.sum(np.log(pred_probs))\n",
    "\n",
    "    best_guess = np.argmax(scores)\n",
    "    return best_guess, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25884c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_guessing_entropy(probs, plaintexts, nibble_idx, true_key_nibble, max_traces=5000):\n",
    "    \"\"\"\n",
    "    Efficient Guessing Entropy calculation using cumulative log-likelihoods.\n",
    "    \"\"\"\n",
    "    n_traces = min(max_traces, len(plaintexts))\n",
    "    log_probs = np.log(np.clip(probs, 1e-10, 1.0))  # shape (n_traces, 16)\n",
    "\n",
    "    # Precompute SBox outputs for each key guess\n",
    "    all_outputs = np.zeros((16, n_traces), dtype=np.uint8)\n",
    "    for guess in range(16):\n",
    "        all_outputs[guess] = np.array([\n",
    "            PRESENT_SBOX[pt[nibble_idx] ^ guess] for pt in plaintexts[:n_traces]\n",
    "        ])\n",
    "\n",
    "    # Cumulative scores over traces\n",
    "    cum_scores = np.zeros((16, n_traces))\n",
    "    for guess in range(16):\n",
    "        guess_indices = all_outputs[guess]\n",
    "        log_likelihoods = log_probs[np.arange(n_traces), guess_indices]\n",
    "        cum_scores[guess] = np.cumsum(log_likelihoods)\n",
    "\n",
    "    # Determine rank of true key guess at each trace count\n",
    "    ranks = []\n",
    "    for t in range(1, n_traces + 1):\n",
    "        sorted_guesses = np.argsort(cum_scores[:, t - 1])[::-1]\n",
    "        rank = np.where(sorted_guesses == true_key_nibble)[0][0]\n",
    "        ranks.append(rank)\n",
    "\n",
    "    return ranks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b37c3568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trace_ranges(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    global_range = tuple(data[\"global\"])\n",
    "\n",
    "    # Convert all per-nibble ranges to list-of-tuples\n",
    "    per_nibble = {\n",
    "        int(k): [tuple(r) for r in v] for k, v in data[\"per_nibble\"].items()\n",
    "    }\n",
    "\n",
    "    return global_range, per_nibble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d16af1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traces_for_ranges(traces, ranges):\n",
    "    if isinstance(ranges, tuple):  # single (start, end)\n",
    "        return traces[:, ranges[0]:ranges[1]]\n",
    "    elif isinstance(ranges, list):  # list of (start, end)\n",
    "        return np.concatenate([traces[:, start:end] for start, end in ranges], axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid trace range format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffea7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_accuracy(model, plaintexts, key_hex, nibble_idx, sbox, traces):\n",
    "    \"\"\"\n",
    "    Evaluate model accuracy on known-key dataset.\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained Keras model for a specific nibble\n",
    "        plaintexts: np.ndarray of shape (n_traces, 8)\n",
    "        key_hex: str, full 16-nibble PRESENT key in hex (e.g. \"FEDCBA0123456789\")\n",
    "        nibble_idx: int, target nibble (0–15)\n",
    "        sbox: list or np.ndarray, PRESENT SBox\n",
    "        traces: np.ndarray of shape (n_traces, n_pois, 1)\n",
    "\n",
    "    Returns:\n",
    "        accuracy: float\n",
    "    \"\"\"\n",
    "    # Parse key into nibbles\n",
    "    key_nibbles = hex_to_nibbles(key_hex)\n",
    "    key_nibble = key_nibbles[nibble_idx]\n",
    "\n",
    "    # Compute true SBox outputs\n",
    "    sbox_outputs = np.array([\n",
    "        sbox[pt[nibble_idx] ^ key_nibble]\n",
    "        for pt in plaintexts\n",
    "    ])\n",
    "\n",
    "    # Predict\n",
    "    probs = model.predict(traces, verbose=0)\n",
    "    predicted_classes = np.argmax(probs, axis=1)\n",
    "\n",
    "    accuracy = np.mean(predicted_classes == sbox_outputs)\n",
    "    print(f\"Accuracy for nibble {nibble_idx}: {accuracy:.4f}\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c9169039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_full_key_w_pois(traces, plaintexts, use_pois=True, dataset_name=\"dataset\"):\n",
    "    \"\"\"\n",
    "    Performs full 16-nibble key recovery using pre-trained per-nibble models.\n",
    "\n",
    "    For each nibble (0–15), the function loads a dedicated model trained to predict\n",
    "    the SBox output given side-channel traces. It supports two evaluation modes:\n",
    "    - POI-based (per-nibble selected points of interest)\n",
    "    - Range-based (per-nibble cropped trace regions)\n",
    "\n",
    "    For each key guess (0–15), the function simulates the expected SBox output using\n",
    "    the known plaintexts and scores the model’s softmax outputs via log-likelihood accumulation.\n",
    "    The most likely key nibble is selected for each position.\n",
    "\n",
    "    The method also compares the recovered key to the known true key, and prints whether\n",
    "    full recovery was successful.\n",
    "\n",
    "    Parameters:\n",
    "        traces (np.ndarray): Power traces of shape (n_traces, n_samples)\n",
    "        plaintexts (np.ndarray): Corresponding plaintexts as 2D byte/nibble arrays\n",
    "        use_pois (bool): Whether to use POI-based models or range-based models\n",
    "\n",
    "    Returns:\n",
    "        None — prints the recovered key, true key, and match status, then plots the\n",
    "        guessing entropy curve for the first nibble.\n",
    "    \"\"\"\n",
    "\n",
    "    full_scores = []\n",
    "    recovered_key = []\n",
    "    guessing_entropy = defaultdict(list)\n",
    "\n",
    "    for idx in range(16):\n",
    "        print(f\"Recovering nibble {idx}...\")\n",
    "        pois_path = f\"{PREPARED_DATASETS_PATH}/pois/nibble_{idx}_pois.npy\"\n",
    "\n",
    "        poi_indices = np.load(pois_path)\n",
    "\n",
    "        _traces = None\n",
    "        _traces = traces.copy()\n",
    "\n",
    "        if use_pois:\n",
    "            _traces = _traces[:, poi_indices]\n",
    "            model = tf.keras.models.load_model(f\"{MODELS_PATH}/per_nibble_POI_n{idx}_aug0.keras\")\n",
    "        else:\n",
    "            trace_ranges_path = os.path.join(PREPARED_DATASETS_PATH, \"ranges.json\")\n",
    "            global_trace_range, per_nibble_trace_ranges = load_trace_ranges(trace_ranges_path)\n",
    "            ranges = per_nibble_trace_ranges[idx]  # list of (start, end)\n",
    "                            \n",
    "            _traces = get_traces_for_ranges(_traces, ranges)\n",
    "            model = tf.keras.models.load_model(f\"{MODELS_PATH}/per_nibble_range_n{idx}_aug0.keras\")\n",
    "        \n",
    "        if len(_traces.shape) == 2:\n",
    "            _traces = np.expand_dims(_traces, axis=-1)\n",
    "        evaluate_model_accuracy(model, plaintexts, KNOWN_KEY_HEX, idx, PRESENT_SBOX, _traces)\n",
    "\n",
    "        # Predict softmax outputs\n",
    "        probs = model.predict(_traces, verbose=0)\n",
    "\n",
    "        best_guess, scores = key_guess(idx, probs, plaintexts, _traces)\n",
    "        \n",
    "        \n",
    "        ranks = compute_guessing_entropy(probs, plaintexts, idx, true_key[idx], max_traces=5000)\n",
    "        label = \"pois\" if use_pois else \"range\"\n",
    "        os.makedirs(f\"ge_data/{dataset_name}/{label}\", exist_ok=True)\n",
    "        np.save(f\"ge_data/{dataset_name}/{label}/ge_nibble_{idx}.npy\", ranks)  # Save to file\n",
    "        guessing_entropy[idx] = ranks\n",
    "\n",
    "        recovered_key.append(best_guess)\n",
    "        full_scores.append(scores)\n",
    "\n",
    "\n",
    "    recovered_key_hex = \"\".join(f\"{k:x}\" for k in recovered_key).upper()\n",
    "    print(\"\\n\\nRecovered Key: \\t\", recovered_key_hex)\n",
    "    print(\"True Key: \\t\", KNOWN_KEY_HEX)\n",
    "    print(\"Key Match:\", recovered_key_hex == KNOWN_KEY_HEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87dbcbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for cache at: ../datasets/cache/random_pt_dataset_cache.pkl\n",
      "Loading datasets from cache: ../datasets/cache/random_pt_dataset_cache.pkl\n",
      "Checking for cache at: ../datasets/cache/fixed_dataset_1_cache.pkl\n",
      "Loading datasets from cache: ../datasets/cache/fixed_dataset_1_cache.pkl\n",
      "Checking for cache at: ../datasets/cache/fixed_dataset_2_cache.pkl\n",
      "Loading datasets from cache: ../datasets/cache/fixed_dataset_2_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "# Load all datasets with caching\n",
    "loaded_datasets = {\n",
    "    name: dataset_loader.load_dataset_files_with_cache(path, cache_path=f\"{DATASETS_PATH}/cache/{name}_cache.pkl\")\n",
    "    for name, path in evaluation_datasets.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487b4bb8",
   "metadata": {},
   "source": [
    "# Evaluation on datasets with per nibble POIs model\n",
    "\n",
    "The models used were the ones trained without augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f9ecdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset, name, use_pois=True):\n",
    "    print(f\"\\n Evaluating on dataset: {name}\")\n",
    "\n",
    "    # Extract traces and plaintexts\n",
    "    traces = dataset_loader.get_trace_matrix(dataset)  # shape (n_traces, n_samples)\n",
    "\n",
    "    plaintexts_raw = dataset[\"plaintexts\"].values\n",
    "    plaintexts = convert_to_hex_string_array(plaintexts_raw)\n",
    "\n",
    "    recover_full_key_w_pois(traces, plaintexts, use_pois=use_pois, dataset_name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f941e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_pt_dataset = loaded_datasets[\"random_pt_dataset\"]\n",
    "fixed_dataset_1 = loaded_datasets[\"fixed_dataset_1\"]\n",
    "fixed_dataset_2 = loaded_datasets[\"fixed_dataset_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bf396577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating on dataset: random_pt_dataset\n",
      "Recovering nibble 0...\n",
      "Accuracy for nibble 0: 0.8640\n",
      "Recovering nibble 1...\n",
      "Accuracy for nibble 1: 0.8762\n",
      "Recovering nibble 2...\n",
      "Accuracy for nibble 2: 0.7872\n",
      "Recovering nibble 3...\n",
      "Accuracy for nibble 3: 0.7792\n",
      "Recovering nibble 4...\n",
      "Accuracy for nibble 4: 0.6950\n",
      "Recovering nibble 5...\n",
      "Accuracy for nibble 5: 0.8900\n",
      "Recovering nibble 6...\n",
      "Accuracy for nibble 6: 0.7826\n",
      "Recovering nibble 7...\n",
      "Accuracy for nibble 7: 0.8708\n",
      "Recovering nibble 8...\n",
      "Accuracy for nibble 8: 0.7914\n",
      "Recovering nibble 9...\n",
      "Accuracy for nibble 9: 0.9126\n",
      "Recovering nibble 10...\n",
      "Accuracy for nibble 10: 0.8884\n",
      "Recovering nibble 11...\n",
      "Accuracy for nibble 11: 0.8920\n",
      "Recovering nibble 12...\n",
      "Accuracy for nibble 12: 0.8050\n",
      "Recovering nibble 13...\n",
      "Accuracy for nibble 13: 0.7704\n",
      "Recovering nibble 14...\n",
      "Accuracy for nibble 14: 0.8392\n",
      "Recovering nibble 15...\n",
      "Accuracy for nibble 15: 0.8360\n",
      "\n",
      "\n",
      "Recovered Key: \t FEDCBA0123456789\n",
      "True Key: \t FEDCBA0123456789\n",
      "Key Match: True\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(random_pt_dataset, \"random_pt_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c3a5193b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating on dataset: fixed_dataset_1\n",
      "Recovering nibble 0...\n",
      "Accuracy for nibble 0: 0.8640\n",
      "Recovering nibble 1...\n",
      "Accuracy for nibble 1: 0.8762\n",
      "Recovering nibble 2...\n",
      "Accuracy for nibble 2: 0.7872\n",
      "Recovering nibble 3...\n",
      "Accuracy for nibble 3: 0.7792\n",
      "Recovering nibble 4...\n",
      "Accuracy for nibble 4: 0.6950\n",
      "Recovering nibble 5...\n",
      "Accuracy for nibble 5: 0.8900\n",
      "Recovering nibble 6...\n",
      "Accuracy for nibble 6: 0.7826\n",
      "Recovering nibble 7...\n",
      "Accuracy for nibble 7: 0.8708\n",
      "Recovering nibble 8...\n",
      "Accuracy for nibble 8: 0.7914\n",
      "Recovering nibble 9...\n",
      "Accuracy for nibble 9: 0.9126\n",
      "Recovering nibble 10...\n",
      "Accuracy for nibble 10: 0.8884\n",
      "Recovering nibble 11...\n",
      "Accuracy for nibble 11: 0.8920\n",
      "Recovering nibble 12...\n",
      "Accuracy for nibble 12: 0.8050\n",
      "Recovering nibble 13...\n",
      "Accuracy for nibble 13: 0.7704\n",
      "Recovering nibble 14...\n",
      "Accuracy for nibble 14: 0.8392\n",
      "Recovering nibble 15...\n",
      "Accuracy for nibble 15: 0.8360\n",
      "\n",
      "\n",
      "Recovered Key: \t FEDCBA0123456789\n",
      "True Key: \t FEDCBA0123456789\n",
      "Key Match: True\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(random_pt_dataset, \"fixed_dataset_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc8d5203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating on dataset: fixed_dataset_2\n",
      "Recovering nibble 0...\n",
      "Accuracy for nibble 0: 0.8640\n",
      "Recovering nibble 1...\n",
      "Accuracy for nibble 1: 0.8762\n",
      "Recovering nibble 2...\n",
      "Accuracy for nibble 2: 0.7872\n",
      "Recovering nibble 3...\n",
      "Accuracy for nibble 3: 0.7792\n",
      "Recovering nibble 4...\n",
      "Accuracy for nibble 4: 0.6950\n",
      "Recovering nibble 5...\n",
      "Accuracy for nibble 5: 0.8900\n",
      "Recovering nibble 6...\n",
      "Accuracy for nibble 6: 0.7826\n",
      "Recovering nibble 7...\n",
      "Accuracy for nibble 7: 0.8708\n",
      "Recovering nibble 8...\n",
      "Accuracy for nibble 8: 0.7914\n",
      "Recovering nibble 9...\n",
      "Accuracy for nibble 9: 0.9126\n",
      "Recovering nibble 10...\n",
      "Accuracy for nibble 10: 0.8884\n",
      "Recovering nibble 11...\n",
      "Accuracy for nibble 11: 0.8920\n",
      "Recovering nibble 12...\n",
      "Accuracy for nibble 12: 0.8050\n",
      "Recovering nibble 13...\n",
      "Accuracy for nibble 13: 0.7704\n",
      "Recovering nibble 14...\n",
      "Accuracy for nibble 14: 0.8392\n",
      "Recovering nibble 15...\n",
      "Accuracy for nibble 15: 0.8360\n",
      "\n",
      "\n",
      "Recovered Key: \t FEDCBA0123456789\n",
      "True Key: \t FEDCBA0123456789\n",
      "Key Match: True\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(random_pt_dataset, \"fixed_dataset_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a93ba75",
   "metadata": {},
   "source": [
    "# Evaluation on datasets with Range POIs per nibble model\n",
    "\n",
    "The models used were the ones trained without augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fb5b1344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating on dataset: random_pt_dataset\n",
      "Recovering nibble 0...\n",
      "Accuracy for nibble 0: 0.9306\n",
      "Recovering nibble 1...\n",
      "Accuracy for nibble 1: 0.9072\n",
      "Recovering nibble 2...\n",
      "Accuracy for nibble 2: 0.5238\n",
      "Recovering nibble 3...\n",
      "Accuracy for nibble 3: 0.8750\n",
      "Recovering nibble 4...\n",
      "Accuracy for nibble 4: 0.0654\n",
      "Recovering nibble 5...\n",
      "Accuracy for nibble 5: 0.0604\n",
      "Recovering nibble 6...\n",
      "Accuracy for nibble 6: 0.0632\n",
      "Recovering nibble 7...\n",
      "Accuracy for nibble 7: 0.9362\n",
      "Recovering nibble 8...\n",
      "Accuracy for nibble 8: 0.8432\n",
      "Recovering nibble 9...\n",
      "Accuracy for nibble 9: 0.7558\n",
      "Recovering nibble 10...\n",
      "Accuracy for nibble 10: 0.0620\n",
      "Recovering nibble 11...\n",
      "Accuracy for nibble 11: 0.8348\n",
      "Recovering nibble 12...\n",
      "Accuracy for nibble 12: 0.8500\n",
      "Recovering nibble 13...\n",
      "Accuracy for nibble 13: 0.7808\n",
      "Recovering nibble 14...\n",
      "Accuracy for nibble 14: 0.8026\n",
      "Recovering nibble 15...\n",
      "Accuracy for nibble 15: 0.9304\n",
      "\n",
      "\n",
      "Recovered Key: \t FEDCD1B123C56789\n",
      "True Key: \t FEDCBA0123456789\n",
      "Key Match: False\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(random_pt_dataset, \"random_pt_dataset\", use_pois=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e0553617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating on dataset: fixed_dataset_1\n",
      "Recovering nibble 0...\n",
      "Accuracy for nibble 0: 0.9306\n",
      "Recovering nibble 1...\n",
      "Accuracy for nibble 1: 0.9072\n",
      "Recovering nibble 2...\n",
      "Accuracy for nibble 2: 0.5238\n",
      "Recovering nibble 3...\n",
      "Accuracy for nibble 3: 0.8750\n",
      "Recovering nibble 4...\n",
      "Accuracy for nibble 4: 0.0654\n",
      "Recovering nibble 5...\n",
      "Accuracy for nibble 5: 0.0604\n",
      "Recovering nibble 6...\n",
      "Accuracy for nibble 6: 0.0632\n",
      "Recovering nibble 7...\n",
      "Accuracy for nibble 7: 0.9362\n",
      "Recovering nibble 8...\n",
      "Accuracy for nibble 8: 0.8432\n",
      "Recovering nibble 9...\n",
      "Accuracy for nibble 9: 0.7558\n",
      "Recovering nibble 10...\n",
      "Accuracy for nibble 10: 0.0620\n",
      "Recovering nibble 11...\n",
      "Accuracy for nibble 11: 0.8348\n",
      "Recovering nibble 12...\n",
      "Accuracy for nibble 12: 0.8500\n",
      "Recovering nibble 13...\n",
      "Accuracy for nibble 13: 0.7808\n",
      "Recovering nibble 14...\n",
      "Accuracy for nibble 14: 0.8026\n",
      "Recovering nibble 15...\n",
      "Accuracy for nibble 15: 0.9304\n",
      "\n",
      "\n",
      "Recovered Key: \t FEDCD1B123C56789\n",
      "True Key: \t FEDCBA0123456789\n",
      "Key Match: False\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(random_pt_dataset, \"fixed_dataset_1\", use_pois=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d946d8be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Evaluating on dataset: fixed_dataset_2\n",
      "Recovering nibble 0...\n",
      "Accuracy for nibble 0: 0.9306\n",
      "Recovering nibble 1...\n",
      "Accuracy for nibble 1: 0.9072\n",
      "Recovering nibble 2...\n",
      "Accuracy for nibble 2: 0.5238\n",
      "Recovering nibble 3...\n",
      "Accuracy for nibble 3: 0.8750\n",
      "Recovering nibble 4...\n",
      "Accuracy for nibble 4: 0.0654\n",
      "Recovering nibble 5...\n",
      "Accuracy for nibble 5: 0.0604\n",
      "Recovering nibble 6...\n",
      "Accuracy for nibble 6: 0.0632\n",
      "Recovering nibble 7...\n",
      "Accuracy for nibble 7: 0.9362\n",
      "Recovering nibble 8...\n",
      "Accuracy for nibble 8: 0.8432\n",
      "Recovering nibble 9...\n",
      "Accuracy for nibble 9: 0.7558\n",
      "Recovering nibble 10...\n",
      "Accuracy for nibble 10: 0.0620\n",
      "Recovering nibble 11...\n",
      "Accuracy for nibble 11: 0.8348\n",
      "Recovering nibble 12...\n",
      "Accuracy for nibble 12: 0.8500\n",
      "Recovering nibble 13...\n",
      "Accuracy for nibble 13: 0.7808\n",
      "Recovering nibble 14...\n",
      "Accuracy for nibble 14: 0.8026\n",
      "Recovering nibble 15...\n",
      "Accuracy for nibble 15: 0.9304\n",
      "\n",
      "\n",
      "Recovered Key: \t FEDCD1B123C56789\n",
      "True Key: \t FEDCBA0123456789\n",
      "Key Match: False\n"
     ]
    }
   ],
   "source": [
    "evaluate_dataset(random_pt_dataset, \"fixed_dataset_2\", use_pois=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114aaaf9",
   "metadata": {},
   "source": [
    "# Conclusion: POI vs. Range POI Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056f0ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2e5df67",
   "metadata": {},
   "source": [
    "The evaluation of per-nibble models on the testing datasets demonstrated a clear advantage of POI-based input selection over range-based cropping. While both approaches achieved high accuracy on several high-leakage nibbles, the range-based models exhibited critical failures on specific nibbles (notably 4, 5, 6, and 10), with accuracy dropping to near-random levels (≈6%). This inconsistency suggests that the pre-defined trace ranges for those nibbles were either misaligned or insufficient to capture the relevant leakage.\n",
    "\n",
    "In contrast, the POI-based models — trained on statistically selected samples using SNR and correlation analysis — maintained consistently high accuracy across nearly all nibbles, with all 16 models correctly recovering the key. The full key match under the POI setting confirms that selective, leakage-aware input extraction yields more robust and generalizable models for key recovery. These results underscore the importance of precise feature selection in profiling side-channel attacks and validate the use of POI clustering as a core part of the attack pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
