{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e05461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 18:31:22.595058: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-08 18:31:23.459552: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-08 18:31:23.879747: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746721884.494540    3098 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746721884.635440    3098 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746721885.593829    3098 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746721885.593896    3098 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746721885.593900    3098 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746721885.593906    3098 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-08 18:31:25.694538: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import utils.dataset_loader as dataset_loader\n",
    "import utils.my_model as model_builder\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189555da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 18:31:40.175077: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81e044f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.get_visible_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a469835e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/miroslav/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxtodorovic\u001b[0m (\u001b[33mmt-thesis\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Optionally fetch the key (for debugging or explicit control)\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# Check if the key is available\n",
    "if wandb_api_key is None:\n",
    "    print(\"WANDB_API_KEY not found in environment variables.\")\n",
    "else:\n",
    "    wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88bea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for cache at: ../datasets/cache/random_dataset_cache.pkl\n",
      "Loading datasets from cache: ../datasets/cache/random_dataset_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "# Paths \n",
    "DATASETS_PATH = '../datasets'\n",
    "random_dataset = dataset_loader.load_dataset_files_with_cache(DATASETS_PATH, cache_path=f\"{DATASETS_PATH}/cache/random_dataset_cache.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb574f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded traces: (10000, 5000)\n"
     ]
    }
   ],
   "source": [
    "traces = dataset_loader.get_trace_matrix(random_dataset)  # shape (n_traces, n_samples)\n",
    "print(f\"Loaded traces: {traces.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebcae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_fixed(traces, labels, model_out_path, augment=False, config=None):\n",
    "    \"\"\"\n",
    "    Train a fixed model using given trace and label data.\n",
    "    \"\"\"\n",
    "    if augment:\n",
    "        traces = traces * np.random.uniform(0.9, 1.1)\n",
    "\n",
    "    if len(traces.shape) == 2:\n",
    "        traces = np.expand_dims(traces, axis=-1)\n",
    "\n",
    "    x_train, x_val, y_train, y_val = train_test_split(\n",
    "        traces, labels, test_size=0.2, shuffle=True, random_state=42\n",
    "    )\n",
    "\n",
    "    input_shape = x_train.shape[1:]\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    print(f\"Input shape: {input_shape}, Number of classes: {num_classes}\")\n",
    "    model = model_builder.build_cnn_model(input_shape[0], num_classes)\n",
    "\n",
    "    run = wandb.init(\n",
    "        entity=\"mt-thesis\",\n",
    "        project=\"model-training\",\n",
    "        name=os.path.splitext(os.path.basename(model_out_path))[0],\n",
    "        config=config,\n",
    "        reinit=True\n",
    "    )\n",
    "    \n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        model.fit(\n",
    "            x_train, y_train,\n",
    "            epochs=1, \n",
    "            batch_size=64, \n",
    "            validation_data=(x_val, y_val),\n",
    "            verbose=1,\n",
    "            callbacks=[\n",
    "                WandbMetricsLogger(),\n",
    "                WandbModelCheckpoint(\n",
    "                    filepath=model_out_path,\n",
    "                    save_best_only=True,\n",
    "                    monitor='val_accuracy',\n",
    "                    mode='max'\n",
    "                ),\n",
    "                lr_scheduler\n",
    "            ])\n",
    "\n",
    "    model.save(model_out_path)\n",
    "    run.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "688bf3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trace_ranges(file_path):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    global_range = tuple(data[\"global\"])\n",
    "\n",
    "    # Convert all per-nibble ranges to list-of-tuples\n",
    "    per_nibble = {\n",
    "        int(k): [tuple(r) for r in v] for k, v in data[\"per_nibble\"].items()\n",
    "    }\n",
    "\n",
    "    return global_range, per_nibble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fe259e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_traces_for_ranges(traces, ranges):\n",
    "    if isinstance(ranges, tuple):  # single (start, end)\n",
    "        return traces[:, ranges[0]:ranges[1]]\n",
    "    elif isinstance(ranges, list):  # list of (start, end)\n",
    "        return np.concatenate([traces[:, start:end] for start, end in ranges], axis=1)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid trace range format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a95ca99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wandb_config(algo, use_pois, poi_type, trace_mode, nibble, augment):\n",
    "    wandb_config = {\n",
    "        \"algorithm\": algo,\n",
    "        \"use_pois\": use_pois,\n",
    "        \"poi_type\": poi_type if use_pois else None,\n",
    "        \"trace_mode\": trace_mode if not use_pois else None,\n",
    "        \"nibble\": nibble,\n",
    "        \"augmentation\": augment\n",
    "    }\n",
    "    return wandb_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0d45f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_trainings(base_dataset_dir, output_dir):\n",
    "    algorithms = [\"PRESENT\", \"AES\"]\n",
    "    use_pois_options = [True, False]\n",
    "    poi_types = [\"global\", \"per_nibble\"]\n",
    "    augmentation_options = [True, False]\n",
    "\n",
    "    for algo in algorithms:\n",
    "        label = \"nibble\" if algo == \"PRESENT\" else \"byte\" \n",
    "        dataset_path = os.path.join(base_dataset_dir, algo.lower())\n",
    "        trace_ranges_path = os.path.join(dataset_path, \"ranges.json\")\n",
    "        global_trace_range, per_nibble_trace_ranges = load_trace_ranges(trace_ranges_path)\n",
    "        for use_pois in use_pois_options:\n",
    "            for augment in augmentation_options:\n",
    "                if use_pois:\n",
    "                    for poi_type in poi_types:\n",
    "                        for idx in range(1):\n",
    "                            print(f\"Training {algo} | POIs={poi_type} | {label}={idx} | Aug={augment}\")\n",
    "                            pois_path = f\"{dataset_path}/pois/{'global' if poi_type == 'global' else f'{label}_{idx}_pois'}.npy\"\n",
    "                            pois = np.load(pois_path)\n",
    "                            _traces = traces.copy()\n",
    "                            _traces = _traces[:, pois]\n",
    "                            print(f\"Loaded POIs: {_traces.shape}\")  # (n_traces, n_pois, 1)\n",
    "                            labels = np.load(f\"{dataset_path}/{label}_{idx}_labels.npy\")\n",
    "\n",
    "                            out_path = f\"{output_dir}/{algo}/{poi_type}_POI_n{idx}_aug{int(augment)}.keras\"\n",
    "                            config = get_wandb_config(algo, use_pois, poi_type, None, idx, augment)\n",
    "                            train_model_fixed(_traces, labels, out_path, augment, config)\n",
    "\n",
    "                else:\n",
    "                    for trace_mode, trace_range in [(\"global\", global_trace_range), (\"per_nibble\", per_nibble_trace_ranges)]:\n",
    "                        for idx in range(1):\n",
    "                            print(f\"Training {algo} | {trace_mode} trace | {label}={idx} | Aug={augment}\")\n",
    "\n",
    "                            if trace_mode == \"per_nibble\":\n",
    "                                ranges = trace_range[idx]  # list of (start, end)\n",
    "                            else:\n",
    "                                ranges = trace_range          # single (start, end)\n",
    "\n",
    "                            # Get the traces for the specified ranges\n",
    "                            _traces = traces.copy()\n",
    "                            trace_subset = get_traces_for_ranges(_traces, ranges)\n",
    "                            print(f\"Subset shape: {trace_subset.shape}\")\n",
    "                            labels = np.load(f\"{dataset_path}/{label}_{idx}_labels.npy\")\n",
    "\n",
    "                            out_path = f\"{output_dir}/{algo}/{trace_mode}_range_n{idx}_aug{int(augment)}.keras\"\n",
    "                            config = get_wandb_config(algo, use_pois, None, trace_mode, idx, augment)\n",
    "                            train_model_fixed(trace_subset, labels, out_path, augment, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfaaac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"../dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be8b71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aa9f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training PRESENT | POIs=global | nibble=0 | Aug=True\n",
      "Loaded POIs: (10000, 702)\n",
      "Training PRESENT | POIs=per_nibble | nibble=0 | Aug=True\n",
      "Loaded POIs: (10000, 47)\n",
      "Training PRESENT | POIs=global | nibble=0 | Aug=False\n",
      "Loaded POIs: (10000, 702)\n",
      "Training PRESENT | POIs=per_nibble | nibble=0 | Aug=False\n",
      "Loaded POIs: (10000, 47)\n",
      "Training PRESENT | global trace | nibble=0 | Aug=True\n",
      "Subset shape: (10000, 2050)\n",
      "Training PRESENT | per_nibble trace | nibble=0 | Aug=True\n",
      "Subset shape: (10000, 350)\n",
      "Training PRESENT | global trace | nibble=0 | Aug=False\n",
      "Subset shape: (10000, 2050)\n",
      "Training PRESENT | per_nibble trace | nibble=0 | Aug=False\n",
      "Subset shape: (10000, 350)\n",
      "Training AES | POIs=global | byte=0 | Aug=True\n",
      "Loaded POIs: (10000, 800)\n",
      "Training AES | POIs=per_nibble | byte=0 | Aug=True\n",
      "Loaded POIs: (10000, 55)\n",
      "Training AES | POIs=global | byte=0 | Aug=False\n",
      "Loaded POIs: (10000, 800)\n",
      "Training AES | POIs=per_nibble | byte=0 | Aug=False\n",
      "Loaded POIs: (10000, 55)\n",
      "Training AES | global trace | byte=0 | Aug=True\n",
      "Subset shape: (10000, 3000)\n",
      "Training AES | per_nibble trace | byte=0 | Aug=True\n",
      "Subset shape: (10000, 420)\n",
      "Training AES | global trace | byte=0 | Aug=False\n",
      "Subset shape: (10000, 3000)\n",
      "Training AES | per_nibble trace | byte=0 | Aug=False\n",
      "Subset shape: (10000, 420)\n"
     ]
    }
   ],
   "source": [
    "run_all_trainings(DATASET_DIR, \"trained\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
