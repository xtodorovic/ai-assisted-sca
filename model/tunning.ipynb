{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e05461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 12:13:20.407764: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-08 12:13:20.417713: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-05-08 12:13:20.446016: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1746699200.483683    5738 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1746699200.495684    5738 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1746699200.527933    5738 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746699200.528012    5738 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746699200.528018    5738 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1746699200.528022    5738 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-08 12:13:20.538400: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard # type: ignore\n",
    "from tensorflow.keras import layers, models # type: ignore\n",
    "from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "import utils.tuner_models as tuner_models\n",
    "import utils.dataset_loader as dataset_loader\n",
    "# Load the .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "189555da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-08 12:13:31.639945: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a469835e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/miroslav/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxtodorovic\u001b[0m (\u001b[33mmt-thesis\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "# Optionally fetch the key (for debugging or explicit control)\n",
    "wandb_api_key = os.getenv(\"WANDB_API_KEY\")\n",
    "\n",
    "# Check if the key is available\n",
    "if wandb_api_key is None:\n",
    "    print(\"WANDB_API_KEY not found in environment variables.\")\n",
    "else:\n",
    "    wandb.login(key=wandb_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a88bea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for cache at: ../datasets/cache/random_dataset_cache.pkl\n",
      "Loading datasets from cache: ../datasets/cache/random_dataset_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "# Paths \n",
    "DATASETS_PATH = '../datasets'\n",
    "random_dataset = dataset_loader.load_dataset_files_with_cache(DATASETS_PATH, cache_path=f\"{DATASETS_PATH}/cache/random_dataset_cache.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb574f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded traces: (10000, 5000)\n"
     ]
    }
   ],
   "source": [
    "traces = dataset_loader.get_trace_matrix(random_dataset)  # shape (n_traces, n_samples)\n",
    "print(f\"Loaded traces: {traces.shape}\")  # (n_traces, n_pois, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f131eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_pois = np.load(os.path.join('../dataset/present/pois', \"nibble_1_pois.npy\"))\n",
    "labels_path = os.path.join('../dataset/present', f\"nibble_0_labels.npy\")\n",
    "\n",
    "labels = np.load(labels_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e15bda52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nibble_models(dataset_dir, build_fn, description=\"advanced_with_nibble_pois\", project_name=\"present-sca\", model_out_dir=\"models/\", nibble_idx=None):\n",
    "    \"\"\"\n",
    "    Train model(s) for PRESENT nibble leakage.\n",
    "\n",
    "    Parameters:\n",
    "        dataset_dir (str): Path to reduced_traces.npy and nibble_X_labels.npy\n",
    "        build_fn (function): Keras model builder (for Keras Tuner)\n",
    "        project_name (str): wandb project name\n",
    "        model_out_dir (str): Where to save models\n",
    "        nibble_idx (int or None): If set, trains only that nibble. If None, trains all 16.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    os.makedirs(model_out_dir, exist_ok=True)\n",
    "\n",
    "    nibble_range = [nibble_idx] if nibble_idx is not None else range(16)\n",
    "\n",
    "    for idx in nibble_range:\n",
    "        print(f\"\\nTraining model for nibble {idx}...\")\n",
    "\n",
    "        selected_pois = np.load(os.path.join('../dataset/present/pois', f\"nibble_{idx}_pois.npy\"))\n",
    "        labels_path = os.path.join('../dataset/present', f\"nibble_{idx}_labels.npy\")\n",
    "\n",
    "        labels = np.load(labels_path)\n",
    "        traces = dataset_loader.get_trace_matrix(random_dataset) \n",
    "        traces = traces[:, selected_pois]\n",
    "\n",
    "        if len(traces.shape) == 2:\n",
    "            traces = np.expand_dims(traces, axis=-1)\n",
    "        print(f\"Loaded traces: {traces.shape}\")  # (n_traces, n_pois, 1)\n",
    "        x_train, x_val, y_train, y_val = train_test_split(traces, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "        trace_length = len(traces[0])\n",
    "        run = wandb.init(\n",
    "            entity=\"mt-thesis\",\n",
    "            project=project_name,\n",
    "            name=f\"nibble_{idx}_cnn_{description}\",\n",
    "            config={\"nibble\": idx},\n",
    "            reinit=True\n",
    "        )\n",
    "\n",
    "        tuner = kt.Hyperband(\n",
    "            build_fn,\n",
    "            objective='val_accuracy',\n",
    "            max_epochs=20,\n",
    "            directory='kerastuner_logs_4',\n",
    "            project_name=f'nibble_{idx}_tuning_cnn_{description}'\n",
    "        )\n",
    "\n",
    "        tuner.search(\n",
    "            x_train, y_train,\n",
    "            validation_data=(x_val, y_val),\n",
    "            epochs=20,\n",
    "            callbacks=[\n",
    "                WandbMetricsLogger(),\n",
    "                WandbModelCheckpoint(filepath=os.path.join(run.dir, \"best_model.keras\"))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        best_model = tuner.get_best_models(num_models=1)[0]\n",
    "        best_model.save(os.path.join(model_out_dir, f\"nibble_{idx}_{description}_model.keras\"))\n",
    "\n",
    "        best_hp = tuner.get_best_hyperparameters(1)[0].values\n",
    "        with open(os.path.join(model_out_dir, f\"nibble_{idx}_{description}_hp.json\"), \"w\") as f:\n",
    "            json.dump(best_hp, f, indent=4)\n",
    "\n",
    "        run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dfaaac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = \"../dataset/present\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be8b71ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa9f791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 26 Complete [00h 03m 20s]\n",
      "val_accuracy: 0.3305000066757202\n",
      "\n",
      "Best val_accuracy So Far: 0.48649999499320984\n",
      "Total elapsed time: 00h 40m 02s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>▁▂▁▁▂▁▁██▁▁▁▁▁▁▂▃▆████▁▁▁▇████▁▁▁▁▁▁▅▆▇█</td></tr><tr><td>epoch/epoch</td><td>▂▁▁▁▁▁▂▃▂▂▂▃▃▂▂▁▂▃▁▂▂▆▄▆▁▃▄▅█▃▇▇▇█▁▄▄▃▄▇</td></tr><tr><td>epoch/learning_rate</td><td>▁▁▂▂▁█▂▁██▂████▁▁▁▁▁▂███████▂▂▂▂███▁▁▁▁▁</td></tr><tr><td>epoch/loss</td><td>▇█▇▇▆▇▇▄▂▇▅▇▁▇▇▇▇▆▅▇▇▇▇▅▂▁▁▇▇▇▁▁█▇▇▇▇▅▄▁</td></tr><tr><td>epoch/val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▄▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr><tr><td>epoch/val_loss</td><td>▁▁▁▁▁▂▆▄▁▂▁▁▁▁▁▁▁▁▁▇▁▁▁▁▁▁▁▃▅█▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch/accuracy</td><td>0.95675</td></tr><tr><td>epoch/epoch</td><td>19</td></tr><tr><td>epoch/learning_rate</td><td>0.0001</td></tr><tr><td>epoch/loss</td><td>0.22225</td></tr><tr><td>epoch/val_accuracy</td><td>0.072</td></tr><tr><td>epoch/val_loss</td><td>13.40919</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nibble_0_cnn_advanced_with_nibble_pois</strong> at: <a href='https://wandb.ai/mt-thesis/present-sca/runs/lzokn84i' target=\"_blank\">https://wandb.ai/mt-thesis/present-sca/runs/lzokn84i</a><br> View project at: <a href='https://wandb.ai/mt-thesis/present-sca' target=\"_blank\">https://wandb.ai/mt-thesis/present-sca</a><br>Synced 5 W&B file(s), 0 media file(s), 372 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250508_121340-lzokn84i/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_nibble_models(DATASET_DIR, tuner_models.build_advanced_cnn, nibble_idx=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
